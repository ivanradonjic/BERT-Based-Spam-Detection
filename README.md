# BERT-Based-Spam-Detection
This repository contains a project leveraging BERT, a state-of-the-art language representation model developed by Google, for classifying spam emails. The project includes a detailed report and a Jupyter Notebook that outline the process of building and evaluating the model. Initially, the dataset of spam and non-spam emails is balanced by downsampling the majority class to ensure effective training. Using pre-trained BERT models from TensorFlow Hub, the project preprocesses and encodes the text data, then constructs a neural network with dropout regularization and a dense layer with a sigmoid activation function for binary classification. The model is trained over 10 epochs using the Adam optimizer and BinaryCrossentropy loss function, achieving high performance metrics with a final binary accuracy of 91.43%, precision of 90.00%, and recall of 93.21%. Evaluation on test data confirms the model's robustness with a binary accuracy of 90.37%, precision of 87.56%, and recall of 94.12%, supported by a detailed confusion matrix and classification report. This project demonstrates BERT's effectiveness in spam email classification, highlighting its potential for practical deployment in enhancing email security and user experience. For more details, refer to the attached report and the Jupyter Notebook.
